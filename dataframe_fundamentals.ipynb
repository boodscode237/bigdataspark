{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc3b0fbe-19e1-487d-82fa-ab30d7157d52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Your First Day:**\n",
    "\n",
    "# DataFrame Fundamentals\n",
    "\n",
    "[Florian Roscheck](http://go.sparkcertcourse.com/X80rbY)\n",
    "\n",
    "This notebook is an exercise in the [Apache Spark Certification Training at SparkCertCourse.com](https://sparkcertcourse.com?utm_source=databricks&utm_medium=notebook_header&utm_campaign=lab1).\n",
    "\n",
    "**üéØ Scope**\n",
    "\n",
    "- Creating DataFrames\n",
    "- Working with schemas\n",
    "- Type conversion of DataFrame columns\n",
    "\n",
    "## Introduction\n",
    "\n",
    "It is your first day as a data analytics intern at DAGMart. Your manager Thiago has just handed you your laptop. He is excited that you join the team, because they really need somebody who can tackle their data challenges with Apache Spark.\n",
    "\n",
    "<div style=\"text-align: center;max-width:600px; margin: 0 auto; margin-top:10px; margin-bottom:10px\">\n",
    "  <img src=\"https://storage.googleapis.com/spark-cert-course/0_spark_cert_course_thiago_welcomes_you.png\" style=\"max-width: 100%; display: inline-block; margin: 0 auto;\", alt=\"A happy manager reaching out to you\">\n",
    "</div>\n",
    "\n",
    "Thiago said that your first project will be something about sustainability data from many different sources. Unfortunately, your sustainability colleague is out of office this week. So, you think about what you can do to prepare for the sustainability project. You come up with the following plan:\n",
    "\n",
    "1. Practice to **create some DataFrames** in PySpark\n",
    "2. Learn more about **schemas**, since they will likely be important for reading data from different sources\n",
    "3. Try to understand **type conversion** and casting, since external data occasionally comes with the wrong types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dfd92c8-125e-454a-8f6b-c9ef56f90964",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Creating DataFrames\n",
    "\n",
    "In this section, you learn how to create DataFrames from scratch in PySpark. Our goal is to create this DataFrame in PySpark:\n",
    "\n",
    "| store_id | size_m2 | revenue_category | owner_name     | country |\n",
    "|----------|---------|------------------|----------------|---------|\n",
    "| 5462     | 68      | 1                | Giovanna Avila | BR      |\n",
    "| 45841    | 74      | 1                | Umang Toor     | IN      |\n",
    "| 2354     | 200     | 1                | Kelsey Jones   | US      |\n",
    "\n",
    "---\n",
    "\n",
    "**üîó Relevant Documentation**\n",
    "- [pyspark.sql.SparkSession.createDataFrame](http://go.sparkcertcourse.com/zsR91Z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd9274d1-f1fb-4817-b021-9e33d7d10a64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define data to create a DataFrame from\n",
    "\n",
    "data = [\n",
    "  (5462, 68, 1, \"Giovanna Avila\", \"BR\"),\n",
    "  (45841, 74, 1, \"Umang Toor\", \"IN\"),\n",
    "  (2354, 200, 1, \"Kelsey Jones\", \"US\")\n",
    "]\n",
    "\n",
    "columns = [\"store_id\", \"size_m2\", \"revenue_category\", \"owner_name\", \"country\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e64f697c-dba2-4123-9b0d-379631a0dd7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>store_id</th><th>size_m2</th><th>revenue_category</th><th>owner_name</th><th>country</th></tr></thead><tbody><tr><td>5462</td><td>68</td><td>1</td><td>Giovanna Avila</td><td>BR</td></tr><tr><td>45841</td><td>74</td><td>1</td><td>Umang Toor</td><td>IN</td></tr><tr><td>2354</td><td>200</td><td>1</td><td>Kelsey Jones</td><td>US</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         5462,
         68,
         1,
         "Giovanna Avila",
         "BR"
        ],
        [
         45841,
         74,
         1,
         "Umang Toor",
         "IN"
        ],
        [
         2354,
         200,
         1,
         "Kelsey Jones",
         "US"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "store_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "size_m2",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "revenue_category",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "owner_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86b5b252-06a5-48b5-8f70-54ab76a64e51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create DataFrame from data defined above\n",
    "df = spark.createDataFrame(data, columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d94b1d2-9d9b-4e34-bda9-1ebfac0dbca5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Express data in dictionary format (you can include column names directly!)\n",
    "\n",
    "data = [\n",
    "  {\n",
    "   \"store_id\": 5462,\n",
    "   \"size_m2\": 68,\n",
    "   \"revenue_category\": 1,\n",
    "   \"owner_name\": \"Giovanna Avila\",\n",
    "   \"country\": \"BR\"\n",
    "  },\n",
    "  {\n",
    "   \"store_id\": 45841,\n",
    "   \"size_m2\": 74,\n",
    "   \"revenue_category\": 1,\n",
    "   \"owner_name\": \"Umang Toor\",\n",
    "   \"country\": \"IN\"\n",
    "  },\n",
    "  {\n",
    "   \"store_id\": 2354,\n",
    "   \"size_m2\": 200,\n",
    "   \"revenue_category\": 1,\n",
    "   \"owner_name\": \"Kelsey Jones\",\n",
    "   \"country\": \"US\"\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3403f08-fbb6-48b9-ae0c-26fb6c1c4ba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>country</th><th>owner_name</th><th>revenue_category</th><th>size_m2</th><th>store_id</th></tr></thead><tbody><tr><td>BR</td><td>Giovanna Avila</td><td>1</td><td>68</td><td>5462</td></tr><tr><td>IN</td><td>Umang Toor</td><td>1</td><td>74</td><td>45841</td></tr><tr><td>US</td><td>Kelsey Jones</td><td>1</td><td>200</td><td>2354</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "BR",
         "Giovanna Avila",
         1,
         68,
         5462
        ],
        [
         "IN",
         "Umang Toor",
         1,
         74,
         45841
        ],
        [
         "US",
         "Kelsey Jones",
         1,
         200,
         2354
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "owner_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "revenue_category",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "size_m2",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "store_id",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate DataFrame from dictionary-formatted data\n",
    "df = spark.createDataFrame(data)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f09c7d56-e861-400f-b184-5fd6df722bee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>size_m2</th>\n",
       "      <th>revenue_category</th>\n",
       "      <th>owner_name</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5462</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>Giovanna Avila</td>\n",
       "      <td>BR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45841</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>Umang Toor</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2354</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>Kelsey Jones</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>store_id</th>\n      <th>size_m2</th>\n      <th>revenue_category</th>\n      <th>owner_name</th>\n      <th>country</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5462</td>\n      <td>68</td>\n      <td>1</td>\n      <td>Giovanna Avila</td>\n      <td>BR</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>45841</td>\n      <td>74</td>\n      <td>1</td>\n      <td>Umang Toor</td>\n      <td>IN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2354</td>\n      <td>200</td>\n      <td>1</td>\n      <td>Kelsey Jones</td>\n      <td>US</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create pandas DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd_df = pd.DataFrame(data)\n",
    "\n",
    "pd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec31426a-ed34-4a68-ae23-a35a6aa5eb92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>store_id</th><th>size_m2</th><th>revenue_category</th><th>owner_name</th><th>country</th></tr></thead><tbody><tr><td>5462</td><td>68</td><td>1</td><td>Giovanna Avila</td><td>BR</td></tr><tr><td>45841</td><td>74</td><td>1</td><td>Umang Toor</td><td>IN</td></tr><tr><td>2354</td><td>200</td><td>1</td><td>Kelsey Jones</td><td>US</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         5462,
         68,
         1,
         "Giovanna Avila",
         "BR"
        ],
        [
         45841,
         74,
         1,
         "Umang Toor",
         "IN"
        ],
        [
         2354,
         200,
         1,
         "Kelsey Jones",
         "US"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "store_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "size_m2",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "revenue_category",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "owner_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate Spark DataFrame from pandas DataFrame\n",
    "\n",
    "df = spark.createDataFrame(pd_df)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "103846ab-e559-46f3-b07d-ea7a4119f797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### ü§ì Exercise: Creating a DataFrame\n",
    "<a id=\"ex_create_df\"></a>\n",
    "Please create a PySpark DataFrame `transactions` with the data shown below. Add one custom row of data. \n",
    "\n",
    "Also make sure the DataFrame has the following columns: \n",
    "\n",
    "| date | transaction_id | store_id | currency |\n",
    "|------|----------------|----------|----------|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b13f1db-d642-4ac5-831d-edb5a83ec805",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Modify this code to yield the DataFrame requested above\n",
    "\n",
    "transactions_data = [\n",
    "  (\"2023-12-09\", \"16a6331b-84b4-4de2-9dc5-fb95621fe823\", 5462, 2.99),\n",
    "  (\"2025-01-04\", \"333d9a36-e0e1-469f-8cd3-acaf8681659a\", 2354, 12.95),\n",
    "  (\"2025-01-05\", \"333d9a36-e0e1-469f-8cd3-acaf8681659a\", 23054, 1.95)\n",
    "]\n",
    "columns = ['date', 'transaction_id', 'store_id', 'currency']\n",
    "transactions =  spark.createDataFrame(transactions_data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03010e97-16e2-4154-8e90-4bf4d06f2785",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>store_id</th><th>size_m2</th><th>revenue_category</th><th>owner_name</th><th>country</th></tr></thead><tbody><tr><td>5462</td><td>68</td><td>1</td><td>Giovanna Avila</td><td>BR</td></tr><tr><td>45841</td><td>74</td><td>1</td><td>Umang Toor</td><td>IN</td></tr><tr><td>2354</td><td>200</td><td>1</td><td>Kelsey Jones</td><td>US</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         5462,
         68,
         1,
         "Giovanna Avila",
         "BR"
        ],
        [
         45841,
         74,
         1,
         "Umang Toor",
         "IN"
        ],
        [
         2354,
         200,
         1,
         "Kelsey Jones",
         "US"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "store_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "size_m2",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "revenue_category",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "owner_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1b72470-a842-4dad-9e89-5b51a1db2016",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./answer_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e52a4ca9-455e-4a79-874b-987bffecfec5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Awesome Spark work!\n"
     ]
    }
   ],
   "source": [
    "check_answer(transactions, exercise='create_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e98c995-cbb6-42b1-acb9-1d5e6a027eb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<span style=\"background-color:#FD6B131A;padding-top:5px;padding-left:10px;padding-right:10px;padding-bottom:5px;display:inline-block;border-radius:10px;border:2px #ffffff33 solid\">Please help me improve the course and click on an emoji to rate this exercise:<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSe2szXH5hep1KzDdMYZ8zdIOM9tVopVwMNL8A1-5e5I0WvAuw/formResponse?usp=pp_url&entry.1715120627=create_df&entry.503063108=1&submit=Submit\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"margin-left:10px;border-top:2px #ca4d02 solid;border-bottom:2px #ca4d02 solid;border-left:2px #ca4d02 solid;border-right:2px #ca4d02 solid;padding-left:7px;padding-right:7px;border-radius:14px;color:#fff;background-color:#fd6b13\">üò©</span></a><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSe2szXH5hep1KzDdMYZ8zdIOM9tVopVwMNL8A1-5e5I0WvAuw/formResponse?usp=pp_url&entry.1715120627=create_df&entry.503063108=2&submit=Submit\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"margin-left:10px;border-top:2px #ca4d02 solid;border-bottom:2px #ca4d02 solid;border-left:2px #ca4d02 solid;border-right:2px #ca4d02 solid;padding-left:7px;padding-right:7px;border-radius:14px;color:#fff;background-color:#fd6b13\">üòü</span></a><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSe2szXH5hep1KzDdMYZ8zdIOM9tVopVwMNL8A1-5e5I0WvAuw/formResponse?usp=pp_url&entry.1715120627=create_df&entry.503063108=3&submit=Submit\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"margin-left:10px;border-top:2px #ca4d02 solid;border-bottom:2px #ca4d02 solid;border-left:2px #ca4d02 solid;border-right:2px #ca4d02 solid;padding-left:7px;padding-right:7px;border-radius:14px;color:#fff;background-color:#fd6b13\">üòê</span></a><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSe2szXH5hep1KzDdMYZ8zdIOM9tVopVwMNL8A1-5e5I0WvAuw/formResponse?usp=pp_url&entry.1715120627=create_df&entry.503063108=4&submit=Submit\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"margin-left:10px;border-top:2px #ca4d02 solid;border-bottom:2px #ca4d02 solid;border-left:2px #ca4d02 solid;border-right:2px #ca4d02 solid;padding-left:7px;padding-right:7px;border-radius:14px;color:#fff;background-color:#fd6b13\">üòä</span></a><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSe2szXH5hep1KzDdMYZ8zdIOM9tVopVwMNL8A1-5e5I0WvAuw/formResponse?usp=pp_url&entry.1715120627=create_df&entry.503063108=5&submit=Submit\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"margin-left:10px;border-top:2px #ca4d02 solid;border-bottom:2px #ca4d02 solid;border-left:2px #ca4d02 solid;border-right:2px #ca4d02 solid;padding-left:7px;padding-right:7px;border-radius:14px;color:#fff;background-color:#fd6b13\">üòÑ</span></a></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1fad768-983f-48cc-af85-41e75cc651ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Working with Schemas\n",
    "\n",
    "In this section, you will learn about working with schemas. Schemas help Spark make sense of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2346973c-2d7f-46b1-a6e5-a6c8f9f3daa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Schemas and (Nullable) Types\n",
    "\n",
    "First, let's have a look at a schema. Then, let's understand how to use certain types and how to make them nullable.\n",
    "\n",
    "---\n",
    "\n",
    "**üîó Relevant Documentation**\n",
    "\n",
    "- [pyspark.sql.DataFrame.printSchema](http://go.sparkcertcourse.com/YqYf7p)\n",
    "- [pyspark.sql.types.StructType](http://go.sparkcertcourse.com/nUOBxM)\n",
    "- [pyspark.sql.types.StructField](http://go.sparkcertcourse.com/qF84iO)\n",
    "- [pyspark.sql.types.LongType](http://go.sparkcertcourse.com/6O3j0S)\n",
    "- [pyspark.sql.types.StringType](http://go.sparkcertcourse.com/4rEuvA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e086b37-a7c2-4296-aba2-44d299dfa187",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>store_id</th><th>size_m2</th><th>revenue_category</th><th>owner_name</th><th>country</th></tr></thead><tbody><tr><td>5462</td><td>68</td><td>1</td><td>Giovanna Avila</td><td>BR</td></tr><tr><td>45841</td><td>74</td><td>1</td><td>Umang Toor</td><td>IN</td></tr><tr><td>2354</td><td>200</td><td>1</td><td>Kelsey Jones</td><td>US</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         5462,
         68,
         1,
         "Giovanna Avila",
         "BR"
        ],
        [
         45841,
         74,
         1,
         "Umang Toor",
         "IN"
        ],
        [
         2354,
         200,
         1,
         "Kelsey Jones",
         "US"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "store_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "size_m2",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "revenue_category",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "owner_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print out schema for DataFrame `df`\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63320e03-1c60-4fcf-b4f0-813eac6243fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "You can see that the schema includes all column names and data types. There is additional information on whether a column can include empty values. This is called \"nullable\". In the case above, all columns can include empty values.\n",
    "\n",
    "Schemas are helpful for informing Spark about which data to expect when reading in a file. It can also help Spark validate and enforce data types.\n",
    "\n",
    "Let's recreate the schema for `df`, but set `revenue_category` so that it cannot be nullable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "693d2c89-a2ce-4013-926f-227eaa31bdac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"display:table;clear:both;border-radius:10px;padding-left:12px;padding-right:12px;border-radius:14px;background-color:#0575BC1A;border:2px #ffffff33 solid;width:48.5em;max-width:100%;display:flex;justify-content:space-between\"><div style=\"float:left;width:1.5em;font-size:1.3em;padding-top:2px;height:100%\">üìö</div><div style=\"flex:1;height:100%;padding-top:7px;padding-bottom:7px;\"><span style=\"color:#0575BC;font-weight:700;display:block\">LEARN MORE</span>We set the theoretical foundation for understanding schemas in the <strong>Data API Deep Dive Lesson</strong>. There, you also find an overview of Spark internal types, all of which you can use in a schema.</div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "834ed371-6fdf-434c-bc7b-acd86a3e5b31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Recreate schema for `df` from scratch, but column `revenue_category` should not be nullable.\n",
    "from pyspark.sql.types import StructType, StructField, LongType, DataType, IntegerType, StringType\n",
    "\n",
    "df_schema = StructType([\n",
    "  StructField('store_id', LongType(), True),\n",
    "  StructField('size_m2', LongType(), True),\n",
    "  StructField('revenue_category', LongType(), False),\n",
    "  StructField('owner_name', StringType(), True),\n",
    "  StructField('country', StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a403c46-2919-4f64-83f4-45fd9b8573f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "You can use schemas to create DataFrames in which types of specific variables are validated and enforced. Let's try with `data` we defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53a1b9be-26ac-40bb-8759-d622480d09ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"display:table;clear:both;border-radius:10px;padding-left:12px;padding-right:12px;border-radius:14px;background-color:#3296091A;border:2px #ffffff33 solid;width:48.5em;max-width:100%;display:flex;justify-content:space-between\"><div style=\"float:left;width:1.5em;font-size:1.3em;padding-top:2px;height:100%\">üí°</div><div style=\"flex:1;height:100%;padding-top:7px;padding-bottom:7px;\"><span style=\"color:#329609;font-weight:700;display:block\">GOOD TO KNOW</span>If you do not supply a schema to <code>spark.createDataFrame()</code> then Spark will try to infer the types of the columns based on the data.</div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84415819-3b43-49ea-9f08-91b71cfc9eec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[25]: [{'store_id': 5462,\n  'size_m2': 68,\n  'revenue_category': 1,\n  'owner_name': 'Giovanna Avila',\n  'country': 'BR'},\n {'store_id': 45841,\n  'size_m2': 74,\n  'revenue_category': 1,\n  'owner_name': 'Umang Toor',\n  'country': 'IN'},\n {'store_id': 2354,\n  'size_m2': 200,\n  'revenue_category': 1,\n  'owner_name': 'Kelsey Jones',\n  'country': 'US'}]"
     ]
    }
   ],
   "source": [
    "# Print out `data` as a reminder\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72ddeb1a-fb1d-4a9d-b55b-d4f2a48824b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>store_id</th><th>size_m2</th><th>revenue_category</th><th>owner_name</th><th>country</th></tr></thead><tbody><tr><td>5462</td><td>68</td><td>1</td><td>Giovanna Avila</td><td>BR</td></tr><tr><td>45841</td><td>74</td><td>1</td><td>Umang Toor</td><td>IN</td></tr><tr><td>2354</td><td>200</td><td>1</td><td>Kelsey Jones</td><td>US</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         5462,
         68,
         1,
         "Giovanna Avila",
         "BR"
        ],
        [
         45841,
         74,
         1,
         "Umang Toor",
         "IN"
        ],
        [
         2354,
         200,
         1,
         "Kelsey Jones",
         "US"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "store_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "size_m2",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "revenue_category",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "owner_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create DataFrame using data in `data` variable and schema `df_schema`\n",
    "df = spark.createDataFrame(data, schema=df_schema)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04a345a6-17c9-4825-85f3-f57daf1d3db4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Make sure that the schema of `df` matches our expectations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "321780cf-af7d-40b0-a17b-c1d67f8a21cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Let's see what happens if we try to add a new row to `data` which does not include a value for `revenue_category`. In the schema, we said that `revenue_category` cannot be nullable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3053200c-806c-4f68-b4ab-f5914322186a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define additional data to add to `data`\n",
    "\n",
    "additional_data = [\n",
    "  {\n",
    "    \"store_id\": 8789,\n",
    "    \"size_m2\": 154,\n",
    "    \"revenue_category\": None,\n",
    "    \"owner_name\": \"Peter Sharick\",\n",
    "    \"country\": \"US\"\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dbd4ee7-77f3-47d6-81a6-0f1a8a81b762",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2378966988586150>:3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Create DataFrame from `data` and `additional_data`, using `df_schema` as a schema\u001B[39;00m\n",
       "\u001B[0;32m----> 3\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(data\u001B[38;5;241m+\u001B[39madditional_data, schema\u001B[38;5;241m=\u001B[39mdf_schema)\n",
       "\u001B[1;32m      4\u001B[0m display(df)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1216\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n",
       "\u001B[1;32m   1211\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n",
       "\u001B[1;32m   1212\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n",
       "\u001B[1;32m   1213\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n",
       "\u001B[1;32m   1214\u001B[0m         data, schema, samplingRatio, verifySchema\n",
       "\u001B[1;32m   1215\u001B[0m     )\n",
       "\u001B[0;32m-> 1216\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_dataframe\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1217\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverifySchema\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n",
       "\u001B[1;32m   1218\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1266\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n",
       "\u001B[1;32m   1264\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromRDD(data\u001B[38;5;241m.\u001B[39mmap(prepare), schema, samplingRatio)\n",
       "\u001B[1;32m   1265\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1266\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_createFromLocal\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mmap\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprepare\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1267\u001B[0m jrdd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSerDeUtil\u001B[38;5;241m.\u001B[39mtoJavaArray(rdd\u001B[38;5;241m.\u001B[39m_to_java_object_rdd())\n",
       "\u001B[1;32m   1268\u001B[0m jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39mapplySchemaToPythonRDD(jrdd\u001B[38;5;241m.\u001B[39mrdd(), struct\u001B[38;5;241m.\u001B[39mjson())\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:888\u001B[0m, in \u001B[0;36mSparkSession._createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n",
       "\u001B[1;32m    880\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_createFromLocal\u001B[39m(\n",
       "\u001B[1;32m    881\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n",
       "\u001B[1;32m    882\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Tuple]\u001B[39m\u001B[38;5;124m\"\u001B[39m, StructType]:\n",
       "\u001B[1;32m    883\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    884\u001B[0m \u001B[38;5;124;03m    Create an RDD for DataFrame from a list or pandas.DataFrame, returns the RDD and schema.\u001B[39;00m\n",
       "\u001B[1;32m    885\u001B[0m \u001B[38;5;124;03m    This would be broken with table acl enabled as user process does not have permission to\u001B[39;00m\n",
       "\u001B[1;32m    886\u001B[0m \u001B[38;5;124;03m    write temp files.\u001B[39;00m\n",
       "\u001B[1;32m    887\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 888\u001B[0m     internal_data, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wrap_data_schema\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    889\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39mparallelize(internal_data), struct\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:858\u001B[0m, in \u001B[0;36mSparkSession._wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n",
       "\u001B[1;32m    853\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_wrap_data_schema\u001B[39m(\n",
       "\u001B[1;32m    854\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n",
       "\u001B[1;32m    855\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Iterable[Tuple], StructType]:\n",
       "\u001B[1;32m    856\u001B[0m     \u001B[38;5;66;03m# make sure data could consumed multiple times\u001B[39;00m\n",
       "\u001B[1;32m    857\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, \u001B[38;5;28mlist\u001B[39m):\n",
       "\u001B[0;32m--> 858\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    860\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n",
       "\u001B[1;32m    861\u001B[0m         struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferSchemaFromList(data, names\u001B[38;5;241m=\u001B[39mschema)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1232\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe.<locals>.prepare\u001B[0;34m(obj)\u001B[0m\n",
       "\u001B[1;32m   1230\u001B[0m \u001B[38;5;129m@no_type_check\u001B[39m\n",
       "\u001B[1;32m   1231\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprepare\u001B[39m(obj):\n",
       "\u001B[0;32m-> 1232\u001B[0m     \u001B[43mverify_func\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1233\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:1867\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify\u001B[0;34m(obj)\u001B[0m\n",
       "\u001B[1;32m   1865\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mverify\u001B[39m(obj: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m   1866\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m verify_nullability(obj):\n",
       "\u001B[0;32m-> 1867\u001B[0m         \u001B[43mverify_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:1835\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify_struct\u001B[0;34m(obj)\u001B[0m\n",
       "\u001B[1;32m   1833\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj, \u001B[38;5;28mdict\u001B[39m):\n",
       "\u001B[1;32m   1834\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m f, verifier \u001B[38;5;129;01min\u001B[39;00m verifiers:\n",
       "\u001B[0;32m-> 1835\u001B[0m         \u001B[43mverifier\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1836\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj, (\u001B[38;5;28mtuple\u001B[39m, \u001B[38;5;28mlist\u001B[39m)):\n",
       "\u001B[1;32m   1837\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(obj) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(verifiers):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:1866\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify\u001B[0;34m(obj)\u001B[0m\n",
       "\u001B[1;32m   1865\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mverify\u001B[39m(obj: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[0;32m-> 1866\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[43mverify_nullability\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m:\n",
       "\u001B[1;32m   1867\u001B[0m         verify_value(obj)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:1723\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify_nullability\u001B[0;34m(obj)\u001B[0m\n",
       "\u001B[1;32m   1721\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   1722\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1723\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(new_msg(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis field is not nullable, but got None\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
       "\u001B[1;32m   1724\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m   1725\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mValueError\u001B[0m: field revenue_category: This field is not nullable, but got None"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\nFile \u001B[0;32m<command-2378966988586150>:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Create DataFrame from `data` and `additional_data`, using `df_schema` as a schema\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(data\u001B[38;5;241m+\u001B[39madditional_data, schema\u001B[38;5;241m=\u001B[39mdf_schema)\n\u001B[1;32m      4\u001B[0m display(df)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1216\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1211\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n\u001B[1;32m   1212\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n\u001B[1;32m   1213\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n\u001B[1;32m   1214\u001B[0m         data, schema, samplingRatio, verifySchema\n\u001B[1;32m   1215\u001B[0m     )\n\u001B[0;32m-> 1216\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_dataframe\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1217\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverifySchema\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m   1218\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1266\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1264\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromRDD(data\u001B[38;5;241m.\u001B[39mmap(prepare), schema, samplingRatio)\n\u001B[1;32m   1265\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1266\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_createFromLocal\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mmap\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprepare\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1267\u001B[0m jrdd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSerDeUtil\u001B[38;5;241m.\u001B[39mtoJavaArray(rdd\u001B[38;5;241m.\u001B[39m_to_java_object_rdd())\n\u001B[1;32m   1268\u001B[0m jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39mapplySchemaToPythonRDD(jrdd\u001B[38;5;241m.\u001B[39mrdd(), struct\u001B[38;5;241m.\u001B[39mjson())\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:888\u001B[0m, in \u001B[0;36mSparkSession._createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m    880\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_createFromLocal\u001B[39m(\n\u001B[1;32m    881\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n\u001B[1;32m    882\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Tuple]\u001B[39m\u001B[38;5;124m\"\u001B[39m, StructType]:\n\u001B[1;32m    883\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    884\u001B[0m \u001B[38;5;124;03m    Create an RDD for DataFrame from a list or pandas.DataFrame, returns the RDD and schema.\u001B[39;00m\n\u001B[1;32m    885\u001B[0m \u001B[38;5;124;03m    This would be broken with table acl enabled as user process does not have permission to\u001B[39;00m\n\u001B[1;32m    886\u001B[0m \u001B[38;5;124;03m    write temp files.\u001B[39;00m\n\u001B[1;32m    887\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 888\u001B[0m     internal_data, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wrap_data_schema\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    889\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39mparallelize(internal_data), struct\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:858\u001B[0m, in \u001B[0;36mSparkSession._wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m    853\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_wrap_data_schema\u001B[39m(\n\u001B[1;32m    854\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n\u001B[1;32m    855\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Iterable[Tuple], StructType]:\n\u001B[1;32m    856\u001B[0m     \u001B[38;5;66;03m# make sure data could consumed multiple times\u001B[39;00m\n\u001B[1;32m    857\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, \u001B[38;5;28mlist\u001B[39m):\n\u001B[0;32m--> 858\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    860\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[1;32m    861\u001B[0m         struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferSchemaFromList(data, names\u001B[38;5;241m=\u001B[39mschema)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1232\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe.<locals>.prepare\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   1230\u001B[0m \u001B[38;5;129m@no_type_check\u001B[39m\n\u001B[1;32m   1231\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprepare\u001B[39m(obj):\n\u001B[0;32m-> 1232\u001B[0m     \u001B[43mverify_func\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1233\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:1867\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   1865\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mverify\u001B[39m(obj: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1866\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m verify_nullability(obj):\n\u001B[0;32m-> 1867\u001B[0m         \u001B[43mverify_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:1835\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify_struct\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   1833\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj, \u001B[38;5;28mdict\u001B[39m):\n\u001B[1;32m   1834\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m f, verifier \u001B[38;5;129;01min\u001B[39;00m verifiers:\n\u001B[0;32m-> 1835\u001B[0m         \u001B[43mverifier\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1836\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj, (\u001B[38;5;28mtuple\u001B[39m, \u001B[38;5;28mlist\u001B[39m)):\n\u001B[1;32m   1837\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(obj) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(verifiers):\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:1866\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   1865\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mverify\u001B[39m(obj: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1866\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[43mverify_nullability\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m   1867\u001B[0m         verify_value(obj)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:1723\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify_nullability\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   1721\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   1722\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1723\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(new_msg(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis field is not nullable, but got None\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m   1724\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1725\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\n\u001B[0;31mValueError\u001B[0m: field revenue_category: This field is not nullable, but got None",
       "errorSummary": "<span class='ansi-red-fg'>ValueError</span>: field revenue_category: This field is not nullable, but got None",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create DataFrame from `data` and `additional_data`, using `df_schema` as a schema\n",
    "\n",
    "df = spark.createDataFrame(data+additional_data, schema=df_schema)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e609d32e-046c-4237-b2b8-44c28d9b576d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "As expected, Spark cannot create `df` because the `revenue_category` contains a null value.\n",
    "\n",
    "When you look at the [documentation](https://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.SparkSession.createDataFrame) of `createDataFrame`, you find the additional parameter `verifySchema`. When you set this flag to `False`, PySpark will avoid verifying the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "628a455a-cf6b-41f3-a5c0-7263abfdd745",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>store_id</th><th>size_m2</th><th>revenue_category</th><th>owner_name</th><th>country</th></tr></thead><tbody><tr><td>5462</td><td>68</td><td>1</td><td>Giovanna Avila</td><td>BR</td></tr><tr><td>45841</td><td>74</td><td>1</td><td>Umang Toor</td><td>IN</td></tr><tr><td>2354</td><td>200</td><td>1</td><td>Kelsey Jones</td><td>US</td></tr><tr><td>8789</td><td>154</td><td>0</td><td>Peter Sharick</td><td>US</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         5462,
         68,
         1,
         "Giovanna Avila",
         "BR"
        ],
        [
         45841,
         74,
         1,
         "Umang Toor",
         "IN"
        ],
        [
         2354,
         200,
         1,
         "Kelsey Jones",
         "US"
        ],
        [
         8789,
         154,
         0,
         "Peter Sharick",
         "US"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "store_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "size_m2",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "revenue_category",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "owner_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set verifySchema to False and try to add the new data again\n",
    "\n",
    "df = spark.createDataFrame(data+additional_data, schema=df_schema, verifySchema=False)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9dcf336a-ca39-4a8e-a6ec-261e537e6e46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "You see now that column `revenue_category` contains the value `0` - the equivalent of `None` expressed as a `LongType`. Different from before, Spark did not complain about the last row having a null value in the `revenue_category` column.\n",
    "\n",
    "Let's now try something different and manually modify the schema. We will change the `revenue_category` column to be nullable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24fcf9fe-28cc-4fad-94de-9f5a8b45bdff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Manually set the revenue_category column to be nullable\n",
    "\n",
    "df_schema['revenue_category'].nullable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b4e56f0-10fe-4267-8c97-a9f2e61ea979",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[34]: StructType([StructField('store_id', LongType(), True), StructField('size_m2', LongType(), True), StructField('revenue_category', LongType(), True), StructField('owner_name', StringType(), True), StructField('country', StringType(), True)])"
     ]
    }
   ],
   "source": [
    "# Let's confirm the column is now correctly set to nullable\n",
    "\n",
    "df_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc54542b-1849-48de-98d2-772efd39661f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "It worked! The `revenue_category` column is now `nullable`.\n",
    "\n",
    "Let's see what happens if we create a DataFrame with the new row that has the empty value in `revenue_category`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "018a5e4c-d7b2-4998-ac0b-511cd970cc78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>store_id</th><th>size_m2</th><th>revenue_category</th><th>owner_name</th><th>country</th></tr></thead><tbody><tr><td>5462</td><td>68</td><td>1</td><td>Giovanna Avila</td><td>BR</td></tr><tr><td>45841</td><td>74</td><td>1</td><td>Umang Toor</td><td>IN</td></tr><tr><td>2354</td><td>200</td><td>1</td><td>Kelsey Jones</td><td>US</td></tr><tr><td>8789</td><td>154</td><td>null</td><td>Peter Sharick</td><td>US</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         5462,
         68,
         1,
         "Giovanna Avila",
         "BR"
        ],
        [
         45841,
         74,
         1,
         "Umang Toor",
         "IN"
        ],
        [
         2354,
         200,
         1,
         "Kelsey Jones",
         "US"
        ],
        [
         8789,
         154,
         null,
         "Peter Sharick",
         "US"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "store_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "size_m2",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "revenue_category",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "owner_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create DataFrame with modified schema\n",
    "\n",
    "df = spark.createDataFrame(data+additional_data, schema=df_schema)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b12d5c48-8235-4fe4-8199-abe2ea5fa0db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Since the column is now nullable, you get a proper `null` value. In the case of `revenue_category`, this could be helpful if you want to differentiate between rows where no value is assigned and a revenue category that would be `0` (along with other revenue categories like `1`, `2`, and so on)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85fd1051-1d2d-4c2f-b574-cce16dc66fbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"display:table;clear:both;border-radius:10px;padding-left:12px;padding-right:12px;border-radius:14px;background-color:#FD6B131A;border:2px #ffffff33 solid;width:48.5em;max-width:100%;display:flex;justify-content:space-between\"><div style=\"float:left;width:1.5em;font-size:1.3em;padding-top:2px;height:100%\">‚ö†Ô∏è</div><div style=\"flex:1;height:100%;padding-top:7px;padding-bottom:7px;\"><span style=\"color:#FD6B13;font-weight:700;display:block\">WATCH OUT!</span>When using types in schemas in PySpark, the type, e.g. <code>StringType</code> <strong>always</strong> needs to be followed by parentheses, so like <code>StringType()</code>. It is important that you understand this for the certification exam.</div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3af273cc-ffa1-4531-b49b-8bab2494623d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Creating Schemas from JSON\n",
    "\n",
    "Did you find the whole `StructType` code cumbersome? In this section, you learn a new trick about how to write schemas in a different format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cf9a651-d531-498a-be40-de4154874df6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[36]: '{\"fields\":[{\"metadata\":{},\"name\":\"store_id\",\"nullable\":true,\"type\":\"long\"},{\"metadata\":{},\"name\":\"size_m2\",\"nullable\":true,\"type\":\"long\"},{\"metadata\":{},\"name\":\"revenue_category\",\"nullable\":true,\"type\":\"long\"},{\"metadata\":{},\"name\":\"owner_name\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"country\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}'"
     ]
    }
   ],
   "source": [
    "# Print out a JSON version of the schema `df_schema`\n",
    "\n",
    "df_schema.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb4ca4e8-0ced-4445-8f8a-9b36c13a6241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "How can you now use this JSON representation? You could write it to a file and use it as a way to communicate the structure of your data to your data science or data engineering colleague. Or, you could convert it to a Python dictionary and circumvent the `StructType` and `StructField` syntax. Let's try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59439ed8-684e-45b0-aa21-a74f71562574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[37]: '{\"fields\":[{\"metadata\":{},\"name\":\"store_id\",\"nullable\":true,\"type\":\"long\"},{\"metadata\":{},\"name\":\"size_m2\",\"nullable\":true,\"type\":\"long\"},{\"metadata\":{},\"name\":\"revenue_category\",\"nullable\":true,\"type\":\"long\"},{\"metadata\":{},\"name\":\"owner_name\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"country\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}'"
     ]
    }
   ],
   "source": [
    "# Store schema from above in a Python string variable\n",
    "\n",
    "schema_json = df_schema.json()\n",
    "schema_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42e9a9f1-cf3e-48cb-b241-cf5c7ea9e73c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[39]: {'fields': [{'metadata': {},\n   'name': 'store_id',\n   'nullable': True,\n   'type': 'long'},\n  {'metadata': {}, 'name': 'size_m2', 'nullable': True, 'type': 'long'},\n  {'metadata': {},\n   'name': 'revenue_category',\n   'nullable': True,\n   'type': 'long'},\n  {'metadata': {}, 'name': 'owner_name', 'nullable': True, 'type': 'string'},\n  {'metadata': {}, 'name': 'country', 'nullable': True, 'type': 'string'}],\n 'type': 'struct'}"
     ]
    }
   ],
   "source": [
    "# Load schema into Python dictionary via Python's json module\n",
    "import json\n",
    "\n",
    "schema_dict = json.loads(schema_json)\n",
    "schema_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ca1d1d5-7564-4dd6-8bd2-75728dfb7e43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"display:table;clear:both;border-radius:10px;padding-left:12px;padding-right:12px;border-radius:14px;background-color:#3296091A;border:2px #ffffff33 solid;width:48.5em;max-width:100%;display:flex;justify-content:space-between\"><div style=\"float:left;width:1.5em;font-size:1.3em;padding-top:2px;height:100%\">üí°</div><div style=\"flex:1;height:100%;padding-top:7px;padding-bottom:7px;\"><span style=\"color:#329609;font-weight:700;display:block\">GOOD TO KNOW</span>The <code>metadata</code> field is a way to store additional metadata about a column in PySpark. This feature can be valuable for keeping a clear and structured description of the data alongside it.</div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68bb3223-c6a1-4122-97f2-434ad80a0443",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Let's now add another field to the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd996254-56ec-4965-8878-0dd7b6aee32b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add field `state` to the schema and make it nullable\n",
    "\n",
    "schema_dict['fields'].append({\"metadata\": {}, \"name\": \"state\", \"nullable\": True, \"type\": \"string\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58d59e1d-6c9d-41a4-8589-2a25148951c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Good, we now have a dictionary-based schema in Python. How do we use it for a PySpark DataFrame? Let's use the `StructType.fromJson()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65a93555-1f1a-455b-ae9b-9b1a2c8430cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>store_id</th><th>size_m2</th><th>revenue_category</th><th>owner_name</th><th>country</th><th>state</th></tr></thead><tbody><tr><td>5462</td><td>68</td><td>1</td><td>Giovanna Avila</td><td>BR</td><td>null</td></tr><tr><td>45841</td><td>74</td><td>1</td><td>Umang Toor</td><td>IN</td><td>null</td></tr><tr><td>2354</td><td>200</td><td>1</td><td>Kelsey Jones</td><td>US</td><td>null</td></tr><tr><td>8789</td><td>154</td><td>null</td><td>Peter Sharick</td><td>US</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         5462,
         68,
         1,
         "Giovanna Avila",
         "BR",
         null
        ],
        [
         45841,
         74,
         1,
         "Umang Toor",
         "IN",
         null
        ],
        [
         2354,
         200,
         1,
         "Kelsey Jones",
         "US",
         null
        ],
        [
         8789,
         154,
         null,
         "Peter Sharick",
         "US",
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "store_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "size_m2",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "revenue_category",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "owner_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Generate the schema from the Python dictionary\n",
    "# 2. Use this schema to create a DataFrame using `data` and `additional_data`\n",
    "# 3. Display the DataFrame\n",
    "\n",
    "schema_json = StructType.fromJson(schema_dict)\n",
    "df = spark.createDataFrame(data+additional_data, schema=schema_json)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b7da62a-3450-4297-8b3d-742c47da734a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- store_id: long (nullable = true)\n |-- size_m2: long (nullable = true)\n |-- revenue_category: long (nullable = true)\n |-- owner_name: string (nullable = true)\n |-- country: string (nullable = true)\n |-- state: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Print the schema to verify that it has been parsed correctly\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba824740-71bf-48d2-97c8-d2749556159e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Perfect, it looks like the schema has been read in correctly and the data was loaded nicely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f921f85-6cc7-4213-bb56-74a52ac5035a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Nested Schemas\n",
    "\n",
    "Reflecting on the work we have done on schemas so far, you may well ask whether the additional effort of defining a schema really is worth it in practice. If the increased robustness of data pipelines through type checking is not a striking argument for you, then maybe here is one: Schemas become very useful when you have complex data that is hard to keep in order.\n",
    "\n",
    "There are certain types like `ArrayType` which allow types, so columns in a DataFrame, to be lists of objects of some other types. What do you think is the content of column `store_hours` as defined below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "472fd201-eddf-4b1b-8e95-c5ae01a2809f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Recreate schema for `df` from scratch, but column `revenue_category` should not be nullable.\n",
    "\n",
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "\n",
    "df_schema = StructType([\n",
    "  StructField(\"store_id\", LongType(), True),\n",
    "  StructField(\"size_m2\", LongType(), True),\n",
    "  StructField(\"revenue_category\", LongType(), False),\n",
    "  StructField(\"owner_name\", StringType(), True),\n",
    "  StructField(\"country\", StringType(), True),\n",
    "  StructField(\"employee_ids\", ArrayType(IntegerType(), False)),\n",
    "  StructField(\"store_hours\", ArrayType(\n",
    "    StructType([\n",
    "      StructField(\"day_of_week\", StringType(), False),\n",
    "      StructField(\"open\", IntegerType(), False),\n",
    "      StructField(\"close\", IntegerType(), False)\n",
    "    ])\n",
    "  ), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f60b56d1-fb57-469f-99c2-63058a6a00f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "`store_hours` is a list of structures with three fields: `day_of_week`, `open`, and `close`. For the data to be valid, all 3 fields need to be included in each entry in `store_hours`, and none of those fields can include a null value. It is acceptable that `store_hours` are not included in the data ‚Äì this field is nullable.\n",
    "\n",
    "Let's write up a nested dataset and feed that into a new DataFrame which follows the nested schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69b301ba-da59-4807-8768-d543b227f869",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create an example of nested data\n",
    "\n",
    "data = [\n",
    "    {'store_id': 5462,\n",
    "     'size_m2': 68,\n",
    "     'revenue_category': 1,\n",
    "     'owner_name': 'Giovanna Avila',\n",
    "     'country': 'BR',\n",
    "     'employee_ids': [989231, 4349923, 439898],\n",
    "     'store_hours': [\n",
    "         {'day_of_week': 'mon', 'open': 8, 'close': 21},\n",
    "         {'day_of_week': 'tue', 'open': 8, 'close': 21},\n",
    "         {'day_of_week': 'wed', 'open': 9, 'close': 17},\n",
    "     ]},\n",
    "    {'store_id': 45841,\n",
    "     'size_m2': 74,\n",
    "     'revenue_category': 1,\n",
    "     'owner_name': 'Umang Toor',\n",
    "     'country': 'IN',\n",
    "     'employee_ids': [231, 312]\n",
    "     },\n",
    "    {'store_id': 2354,\n",
    "     'size_m2': 200,\n",
    "     'revenue_category': 1,\n",
    "     'owner_name': 'Kelsey Jones',\n",
    "     'country': 'US',\n",
    "     'employee_ids': [4564, 2548, 8795, 5487, 6548, 5489],\n",
    "     'store_hours': [\n",
    "         {'day_of_week': 'mon', 'open': 7, 'close': 22},\n",
    "         {'day_of_week': 'tue', 'open': 7, 'close': 22},\n",
    "         {'day_of_week': 'wed', 'open': 7, 'close': 22},\n",
    "     ]}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b68419bf-ac07-43cc-b51b-02038ac8a6e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>store_id</th><th>size_m2</th><th>revenue_category</th><th>owner_name</th><th>country</th><th>employee_ids</th><th>store_hours</th></tr></thead><tbody><tr><td>5462</td><td>68</td><td>1</td><td>Giovanna Avila</td><td>BR</td><td>List(989231, 4349923, 439898)</td><td>List(List(mon, 8, 21), List(tue, 8, 21), List(wed, 9, 17))</td></tr><tr><td>45841</td><td>74</td><td>1</td><td>Umang Toor</td><td>IN</td><td>List(231, 312)</td><td>null</td></tr><tr><td>2354</td><td>200</td><td>1</td><td>Kelsey Jones</td><td>US</td><td>List(4564, 2548, 8795, 5487, 6548, 5489)</td><td>List(List(mon, 7, 22), List(tue, 7, 22), List(wed, 7, 22))</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         5462,
         68,
         1,
         "Giovanna Avila",
         "BR",
         [
          989231,
          4349923,
          439898
         ],
         [
          [
           "mon",
           8,
           21
          ],
          [
           "tue",
           8,
           21
          ],
          [
           "wed",
           9,
           17
          ]
         ]
        ],
        [
         45841,
         74,
         1,
         "Umang Toor",
         "IN",
         [
          231,
          312
         ],
         null
        ],
        [
         2354,
         200,
         1,
         "Kelsey Jones",
         "US",
         [
          4564,
          2548,
          8795,
          5487,
          6548,
          5489
         ],
         [
          [
           "mon",
           7,
           22
          ],
          [
           "tue",
           7,
           22
          ],
          [
           "wed",
           7,
           22
          ]
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "store_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "size_m2",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "revenue_category",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "owner_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_ids",
         "type": "{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false}"
        },
        {
         "metadata": "{}",
         "name": "store_hours",
         "type": "{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"day_of_week\",\"type\":\"string\",\"nullable\":false,\"metadata\":{}},{\"name\":\"open\",\"type\":\"integer\",\"nullable\":false,\"metadata\":{}},{\"name\":\"close\",\"type\":\"integer\",\"nullable\":false,\"metadata\":{}}]},\"containsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate a DataFrame from the nested data and the schema and display it\n",
    "\n",
    "df = spark.createDataFrame(data, schema=df_schema)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8f56a69-7fa6-49b0-aeaa-728a109dd6ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Play around a little with the rendering of the data in the Databricks notebook. Being able to collapse and expand the entries in the `store_hours` can be quite useful to not lose the big picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ae0f3d9-8603-4964-bfca-8d554d270fcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### ü§ì Exercise: Building a Simple Schema\n",
    "<a id=\"ex_simple_schema\"></a>\n",
    "\n",
    "<div style=\"text-align: center;max-width:600px; margin: 0 auto; margin-top:10px; margin-bottom:10px\">\n",
    "  <img src=\"https://storage.googleapis.com/spark-cert-course/1_spark_cert_course_lunch_with_katherine.png\" style=\"max-width: 100%; display: inline-block; margin: 0 auto;\", alt=\"woman with glasses and red-brown hair having lunch with you in an office canteen\">\n",
    "</div>\n",
    "\n",
    "For lunch, your boss Thiago set you up with Katherine. Katherine works in accounting and always loves to welcome new data nerds to DAGMart. She tells you about a new big data pull that they need to do for a billing dispute with a distributor. On a napkin, she sketches out:\n",
    "\n",
    "```\n",
    "*DATA STRUCTURE FOR DATA PULL*\n",
    "\n",
    "‚îå‚îÄ PurchaseDate     (should be a date, no exact time required)\n",
    "‚îÇ\n",
    "‚îú‚îÄ HasPaid          (either true or false)\n",
    "‚îÇ\n",
    "‚îî‚îÄ HasReceivedGood  (either true or false, but only filled if shipped)\n",
    "\n",
    "```\n",
    "\n",
    "You tell her that what she drew out is called a schema which of course she already knows. The lunch was great and you return to your desks.\n",
    "\n",
    "To put your knowledge into practice, you want to sketch out a schema which would help reflect the data in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbc75358-d3de-4736-a249-e885fdf42e16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# You come up with the following data to test your schema\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "simple_schema_data = [\n",
    "  (date(2023, 9, 12), True, True),\n",
    "  (date(2023, 4, 28), True, False),\n",
    "  (date(2022, 12, 10), False, None),\n",
    "  (date(2022, 10, 1), False, None)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e76a33a9-f575-420d-9dba-f24b538e7365",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, DateType, BooleanType\n",
    "\n",
    "simple_schema = StructType([\n",
    "  StructField(\"PurchaseDate\", DateType(), False),\n",
    "  StructField('HasPaid', BooleanType(), False),\n",
    "  StructField('HasReceivedGood', BooleanType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54047be8-57ba-4b3a-b075-f8cd4a935628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./answer_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c305373f-d0fd-4f06-9afe-c6d028791dbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Nice attempt, let's try again! There is an error with the data type of column PurchaseDate.\n"
     ]
    }
   ],
   "source": [
    "check_answer(simple_schema, exercise='simple_schema')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dcb7c8d-13fe-4d8d-9f81-e2beacbf7dbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>PurchaseDate</th><th>HasPaid</th><th>HasReceivedGood</th></tr></thead><tbody><tr><td>2023-09-12</td><td>true</td><td>true</td></tr><tr><td>2023-04-28</td><td>true</td><td>false</td></tr><tr><td>2022-12-10</td><td>false</td><td>null</td></tr><tr><td>2022-10-01</td><td>false</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2023-09-12",
         true,
         true
        ],
        [
         "2023-04-28",
         true,
         false
        ],
        [
         "2022-12-10",
         false,
         null
        ],
        [
         "2022-10-01",
         false,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "PurchaseDate",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "HasPaid",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "HasReceivedGood",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a DataFrame from `simple_schema` and `simple_schema_data` and display it\n",
    "\n",
    "simple_schema_df = spark.createDataFrame(simple_schema_data, schema=simple_schema)\n",
    "display(simple_schema_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5277fd7e-8a1f-4b27-ba0d-558069fd3e68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Oops, not the right answer, but keep trying! The schema of the DataFrame seems to be incorrect\n"
     ]
    }
   ],
   "source": [
    "check_answer(simple_schema_df, exercise='simple_schema_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff9a3784-588a-4dc5-8999-0a2063d96fcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<span style=\"background-color:#FD6B131A;padding-top:5px;padding-left:10px;padding-right:10px;padding-bottom:5px;display:inline-block;border-radius:10px;border:2px #ffffff33 solid\">Please help me improve the course and click on an emoji to rate this exercise:<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSe2szXH5hep1KzDdMYZ8zdIOM9tVopVwMNL8A1-5e5I0WvAuw/formResponse?usp=pp_url&entry.1715120627=simple_schema&entry.503063108=1&submit=Submit\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"margin-left:10px;border-top:2px #ca4d02 solid;border-bottom:2px #ca4d02 solid;border-left:2px #ca4d02 solid;border-right:2px #ca4d02 solid;padding-left:7px;padding-right:7px;border-radius:14px;color:#fff;background-color:#fd6b13\">üò©</span></a><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSe2szXH5hep1KzDdMYZ8zdIOM9tVopVwMNL8A1-5e5I0WvAuw/formResponse?usp=pp_url&entry.1715120627=simple_schema&entry.503063108=2&submit=Submit\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"margin-left:10px;border-top:2px #ca4d02 solid;border-bottom:2px #ca4d02 solid;border-left:2px #ca4d02 solid;border-right:2px #ca4d02 solid;padding-left:7px;padding-right:7px;border-radius:14px;color:#fff;background-color:#fd6b13\">üòü</span></a><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSe2szXH5hep1KzDdMYZ8zdIOM9tVopVwMNL8A1-5e5I0WvAuw/formResponse?usp=pp_url&entry.1715120627=simple_schema&entry.503063108=3&submit=Submit\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"margin-left:10px;border-top:2px #ca4d02 solid;border-bottom:2px #ca4d02 solid;border-left:2px #ca4d02 solid;border-right:2px #ca4d02 solid;padding-left:7px;padding-right:7px;border-radius:14px;color:#fff;background-color:#fd6b13\">üòê</span></a><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSe2szXH5hep1KzDdMYZ8zdIOM9tVopVwMNL8A1-5e5I0WvAuw/formResponse?usp=pp_url&entry.1715120627=simple_schema&entry.503063108=4&submit=Submit\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"margin-left:10px;border-top:2px #ca4d02 solid;border-bottom:2px #ca4d02 solid;border-left:2px #ca4d02 solid;border-right:2px #ca4d02 solid;padding-left:7px;padding-right:7px;border-radius:14px;color:#fff;background-color:#fd6b13\">üòä</span></a><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSe2szXH5hep1KzDdMYZ8zdIOM9tVopVwMNL8A1-5e5I0WvAuw/formResponse?usp=pp_url&entry.1715120627=simple_schema&entry.503063108=5&submit=Submit\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"margin-left:10px;border-top:2px #ca4d02 solid;border-bottom:2px #ca4d02 solid;border-left:2px #ca4d02 solid;border-right:2px #ca4d02 solid;padding-left:7px;padding-right:7px;border-radius:14px;color:#fff;background-color:#fd6b13\">üòÑ</span></a></span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "689d77be-c38e-4bde-9503-3a1ecede2a72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### ü§ì Exercise: Building a Complex Schema\n",
    "<a id=\"ex_build_complex_schema\"></a>\n",
    "\n",
    "You could not help yourself and ended up sending your schema to Katherine via your company-internal chat. She is impressed with your effort and wants to challenge you a little.\n",
    "\n",
    "<div style=\"text-align: center;max-width:600px; margin: 0 auto; margin-top:10px; margin-bottom:10px\">\n",
    "  <img src=\"https://storage.googleapis.com/spark-cert-course/2_spark_cert_course_chat_with_katherine.png\" style=\"max-width: 100%; display: inline-block; margin: 0 auto;\", alt=\"woman with glasses and red-brown hair on the screen of a laptop in a chat program\">\n",
    "</div>\n",
    "\n",
    "There is a problem she was not able to find time to solve yet. She gives you some information in the chat and you extend the napkin drawing:\n",
    "\n",
    "```\n",
    "*SCHEMA FOR DATA PULL*\n",
    "\n",
    "‚îå‚îÄ PurchaseDate     (should be a date, no exact time required)\n",
    "‚îÇ\n",
    "‚îú‚îÄ HasPaid          (either true or false)\n",
    "‚îÇ\n",
    "‚îú‚îÄ HasReceivedGood  (either true or false, but only filled if shipped)\n",
    "‚îÇ\n",
    "‚îÇ                ‚îå‚îÄ TrackingNumber   (12 digits, numbers only)\n",
    "‚îî‚îÄ TrackingInfo‚îÄ‚îÄ‚î§\n",
    "    .            ‚îî‚îÄ TrackingProvider  (just name of provider)\n",
    "    .\n",
    "    ..(only filled if shipped, can have multiple shipments with\n",
    "       separate tracking details) \n",
    "```\n",
    "\n",
    "Can you add the additional information about tracking to the schema? You will have to import new types to complete the schema. Please use an appropriate numeric type for `TrackingNumber`. You can find more information about available types in the [Spark documentation](https://go.sparkcertcourse.com/RJuQoR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "493e8824-2f9e-4032-b117-0db6b26006db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# You come up with the following data to test your schema\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "complex_schema_data = [\n",
    "  (date(2023, 9, 12), True, True, [[389293891238, \"QuickMail\"]]),\n",
    "  (date(2023, 4, 28), True, False, [[923881992039, \"E2HL\"], [291823782930, \"E2HL\"]]),\n",
    "  (date(2022, 12, 10), False, None, None),\n",
    "  (date(2022, 10, 1), False, None, None)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8da4346a-b7ed-47f2-ad6a-c5248e47f5b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StringType, StructField, IntegerType, DateType, BooleanType, ArrayType, LongType\n",
    "\n",
    "complex_schema = StructType([\n",
    "  StructField(\"PurchaseDate\", DateType(), False),\n",
    "  StructField('HasPaid', BooleanType(), False),\n",
    "  StructField('HasReceivedGood', BooleanType(), True),\n",
    "  StructField(\"TrackingNumber\", ArrayType(\n",
    "    StructType([\n",
    "      StructField(\"TrackingNumber\", LongType(), False),\n",
    "      StructField(\"TrackingProvider\", StringType(), False)\n",
    "    ])\n",
    "  ), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab52e593-f0e9-40df-9eba-ca19e2592e90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./answer_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccb10400-bbef-41a5-b7bc-d1cd1ab91791",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå That's not quite right, but don't be discouraged! The name of the 4th column differs from the napkin sketch.\n"
     ]
    }
   ],
   "source": [
    "check_answer(complex_schema, exercise='complex_schema')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12329b84-af05-4360-86cc-0b72017d2b7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>PurchaseDate</th><th>HasPaid</th><th>HasReceivedGood</th><th>TrackingNumber</th></tr></thead><tbody><tr><td>2023-09-12</td><td>true</td><td>true</td><td>List(List(389293891238, QuickMail))</td></tr><tr><td>2023-04-28</td><td>true</td><td>false</td><td>List(List(923881992039, E2HL), List(291823782930, E2HL))</td></tr><tr><td>2022-12-10</td><td>false</td><td>null</td><td>null</td></tr><tr><td>2022-10-01</td><td>false</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2023-09-12",
         true,
         true,
         [
          [
           389293891238,
           "QuickMail"
          ]
         ]
        ],
        [
         "2023-04-28",
         true,
         false,
         [
          [
           923881992039,
           "E2HL"
          ],
          [
           291823782930,
           "E2HL"
          ]
         ]
        ],
        [
         "2022-12-10",
         false,
         null,
         null
        ],
        [
         "2022-10-01",
         false,
         null,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "PurchaseDate",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "HasPaid",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "HasReceivedGood",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "TrackingNumber",
         "type": "{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"TrackingNumber\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"TrackingProvider\",\"type\":\"string\",\"nullable\":false,\"metadata\":{}}]},\"containsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the DataFrame here once you completed the exercise\n",
    "\n",
    "complex_schema_df = spark.createDataFrame(complex_schema_data, schema=complex_schema)\n",
    "display(complex_schema_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3cc59b5-3bec-463d-8dfb-c9977095a2cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<span style=\"background-color:#FD6B131A;padding-top:5px;padding-left:10px;padding-right:10px;padding-bottom:5px;display:inline-block;border-radius:10px;border:2px #ffffff33 solid\">Please help me improve the course and click on an emoji to rate this exercise:<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSe2szXH5hep1KzDdMYZ8zdIOM9tVopVwMNL8A1-5e5I0WvAuw/formResponse?usp=pp_url&entry.1715120627=complex_schema&entry.503063108=1&submit=Submit\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"margin-left:10px;border-top:2px #ca4d02 solid;border-bottom:2px #ca4d02 solid;border-left:2px #ca4d02 solid;border-right:2px #ca4d02 solid;padding-left:7px;padding-right:7px;border-radius:14px;color:#fff;background-color:#fd6b13\">üò©</span></a><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSe2szXH5hep1KzDdMYZ8zdIOM9tVopVwMNL8A1-5e5I0WvAuw/formResponse?usp=pp_url&entry.1715120627=complex_schema&entry.503063108=2&submit=Submit\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"margin-left:10px;border-top:2px #ca4d02 solid;border-bottom:2px #ca4d02 solid;border-left:2px #ca4d02 solid;border-right:2px #ca4d02 solid;padding-left:7px;padding-right:7px;border-radius:14px;color:#fff;background-color:#fd6b13\">üòü</span></a><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSe2szXH5hep1KzDdMYZ8zdIOM9tVopVwMNL8A1-5e5I0WvAuw/formResponse?usp=pp_url&entry.1715120627=complex_schema&entry.503063108=3&submit=Submit\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"margin-left:10px;border-top:2px #ca4d02 solid;border-bottom:2px #ca4d02 solid;border-left:2px #ca4d02 solid;border-right:2px #ca4d02 solid;padding-left:7px;padding-right:7px;border-radius:14px;color:#fff;background-color:#fd6b13\">üòê</span></a><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSe2szXH5hep1KzDdMYZ8zdIOM9tVopVwMNL8A1-5e5I0WvAuw/formResponse?usp=pp_url&entry.1715120627=complex_schema&entry.503063108=4&submit=Submit\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"margin-left:10px;border-top:2px #ca4d02 solid;border-bottom:2px #ca4d02 solid;border-left:2px #ca4d02 solid;border-right:2px #ca4d02 solid;padding-left:7px;padding-right:7px;border-radius:14px;color:#fff;background-color:#fd6b13\">üòä</span></a><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSe2szXH5hep1KzDdMYZ8zdIOM9tVopVwMNL8A1-5e5I0WvAuw/formResponse?usp=pp_url&entry.1715120627=complex_schema&entry.503063108=5&submit=Submit\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"margin-left:10px;border-top:2px #ca4d02 solid;border-bottom:2px #ca4d02 solid;border-left:2px #ca4d02 solid;border-right:2px #ca4d02 solid;padding-left:7px;padding-right:7px;border-radius:14px;color:#fff;background-color:#fd6b13\">üòÑ</span></a></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d47fd933-10d5-4ee3-a8ee-20d0422cba1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Type Conversion of DataFrame Columns\n",
    "\n",
    "In this section, you will learn how to convert types of DataFrame columns and get to know more about the topic of type casting in Spark.\n",
    "\n",
    "---\n",
    "\n",
    "**üîó Relevant Documentation**\n",
    "\n",
    "- [pyspark.sql.DataFrame.withColumn](https://go.sparkcertcourse.com/KCfgFV)\n",
    "- [pyspark.sql.functions.col](https://go.sparkcertcourse.com/sI6dD8)\n",
    "- [pyspark.sql.Column](https://go.sparkcertcourse.com/bcbJKY)\n",
    "- [pyspark.sql.Column.cast](https://go.sparkcertcourse.com/PieS6y)\n",
    "- [pyspark.sql.types.FloatType](https://go.sparkcertcourse.com/8wpoJY)\n",
    "- [pyspark.sql.DataFrame.withColumnRenamed](https://go.sparkcertcourse.com/9Y4fKq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e36f365-0400-4a86-b6c3-22edd53e7eb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>store_id</th><th>size_m2</th><th>revenue_category</th><th>owner_name</th><th>country</th></tr></thead><tbody><tr><td>5462</td><td>68</td><td>1</td><td>Giovanna Avila</td><td>BR</td></tr><tr><td>45841</td><td>74</td><td>1</td><td>Umang Toor</td><td>IN</td></tr><tr><td>2354</td><td>200</td><td>1</td><td>Kelsey Jones</td><td>US</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         5462,
         68,
         1,
         "Giovanna Avila",
         "BR"
        ],
        [
         45841,
         74,
         1,
         "Umang Toor",
         "IN"
        ],
        [
         2354,
         200,
         1,
         "Kelsey Jones",
         "US"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "store_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "size_m2",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "revenue_category",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "owner_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define data to create a DataFrame from\n",
    "\n",
    "data = [\n",
    "  (5462, 68, 1, \"Giovanna Avila\", \"BR\"),\n",
    "  (45841, 74, 1, \"Umang Toor\", \"IN\"),\n",
    "  (2354, 200, 1, \"Kelsey Jones\", \"US\")\n",
    "]\n",
    "\n",
    "columns = [\"store_id\", \"size_m2\", \"revenue_category\", \"owner_name\", \"country\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ae91deb-afbf-4574-88d2-496461f198dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- store_id: long (nullable = true)\n |-- size_m2: long (nullable = true)\n |-- revenue_category: long (nullable = true)\n |-- owner_name: string (nullable = true)\n |-- country: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Show schema for reference\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60949444-b943-4110-9a03-7c75312a81d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Now that we have created a DataFrame to work with, let's start converting types of columns. We will use three new commands here: `DataFrame.withColumn`, `col` and `cast`.\n",
    "\n",
    "We use [pyspark.sql.functions.col](https://go.sparkcertcourse.com/sI6dD8) to return a [pyspark.sql.Column](https://go.sparkcertcourse.com/bcbJKY) object. A Column object refers to a specific column in a DataFrame. It offers a lot of different methods to **operate on data in that specific column**.\n",
    "\n",
    "Since we want to **change the data type** of that column, we can use the [pyspark.sql.Column.cast](https://go.sparkcertcourse.com/PieS6y) method to do so.\n",
    "\n",
    "To **add a column** to a DataFrame, there is the [pyspark.sql.DataFrame.withColumn](https://go.sparkcertcourse.com/KCfgFV) method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2380d1ae-7dc9-4307-8f73-f2d84d3337db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>store_id</th><th>size_m2</th><th>revenue_category</th><th>owner_name</th><th>country</th><th>size_m2_long</th></tr></thead><tbody><tr><td>5462</td><td>68</td><td>1</td><td>Giovanna Avila</td><td>BR</td><td>68.0</td></tr><tr><td>45841</td><td>74</td><td>1</td><td>Umang Toor</td><td>IN</td><td>74.0</td></tr><tr><td>2354</td><td>200</td><td>1</td><td>Kelsey Jones</td><td>US</td><td>200.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         5462,
         68,
         1,
         "Giovanna Avila",
         "BR",
         68.0
        ],
        [
         45841,
         74,
         1,
         "Umang Toor",
         "IN",
         74.0
        ],
        [
         2354,
         200,
         1,
         "Kelsey Jones",
         "US",
         200.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "store_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "size_m2",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "revenue_category",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "owner_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size_m2_long",
         "type": "\"float\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a new DataFrame `df_cast` which is like `df` but\n",
    "# with an additional column `size_m2_long`\n",
    "# in which column `size_m2` from `df` is cast to FloatType.\n",
    "# Then, display `df_cast`.\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "df_cast = df.withColumn('size_m2_long', col('size_m2').cast(FloatType()))\n",
    "display(df_cast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e42ff06-061f-482c-b77c-ab2b21b9e62b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- store_id: long (nullable = true)\n |-- size_m2: long (nullable = true)\n |-- revenue_category: long (nullable = true)\n |-- owner_name: string (nullable = true)\n |-- country: string (nullable = true)\n |-- size_m2_long: float (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Print schema of `df_cast` to confirm changes.\n",
    "\n",
    "df_cast.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c4427e9-e22c-4626-b807-724f4bc8f039",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Nice, the additional column `size_m2_long` is of type `float` ‚Äì just as we wanted.\n",
    "\n",
    "If importing `FloatType` is too verbose for you and you would like to take a shortcut, you can also pass a string with the desired type into `Column.cast()`, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a129f32-fc4d-4878-b9d6-549ad2fdf5db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>store_id</th><th>size_m2</th><th>revenue_category</th><th>owner_name</th><th>country</th><th>size_m2_long</th></tr></thead><tbody><tr><td>5462</td><td>68</td><td>1</td><td>Giovanna Avila</td><td>BR</td><td>68.0</td></tr><tr><td>45841</td><td>74</td><td>1</td><td>Umang Toor</td><td>IN</td><td>74.0</td></tr><tr><td>2354</td><td>200</td><td>1</td><td>Kelsey Jones</td><td>US</td><td>200.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         5462,
         68,
         1,
         "Giovanna Avila",
         "BR",
         68.0
        ],
        [
         45841,
         74,
         1,
         "Umang Toor",
         "IN",
         74.0
        ],
        [
         2354,
         200,
         1,
         "Kelsey Jones",
         "US",
         200.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "store_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "size_m2",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "revenue_category",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "owner_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size_m2_long",
         "type": "\"float\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define type for casting as string instead of as type from pyspark.sql.types\n",
    "\n",
    "df_cast = df.withColumn('size_m2_long', col('size_m2').cast('float'))\n",
    "display(df_cast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "165e259f-49e9-4e1f-93d4-7e5d19f0a85a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "You can see that this works just as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18e02bd2-5b44-4400-9e24-66d4e9068555",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### ü§ì Exercise: Changing the Type of a Column\n",
    "<a id=\"ex_change_col_type\"></a>\n",
    "\n",
    "After having worked on Katherine's problem, you are now back to yourself. Your first work day is going great so far and you decide to dabble around with column type conversions ‚Äì something that inevitably will come up when you deal with DAGMart's data.\n",
    "\n",
    "<div style=\"text-align: center;max-width:600px; margin: 0 auto; margin-top:10px; margin-bottom:10px\">\n",
    "  <img src=\"https://storage.googleapis.com/spark-cert-course/3_spark_cert_course_working_by_yourself.png\" style=\"max-width: 100%; display: inline-block; margin: 0 auto;\", alt=\"a laptop on an office desk\">\n",
    "</div>\n",
    "\n",
    "You want to change the type of an existing column in the DataFrame you used when defining your first simple schema. \n",
    "\n",
    "Can you create a new DataFrame from `simple_schema_df` so that column `PurchaseDate` is of `StringType` and `HasPaid` is of `ByteType`? To apply multiple commands to a DataFrame, you can just chain them like `DataFrame.dofirst().dosecond().dothird()` and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2991301-15b0-4a67-91ae-087d093dcebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"display:table;clear:both;border-radius:10px;padding-left:12px;padding-right:12px;border-radius:14px;background-color:#3296091A;border:2px #ffffff33 solid;width:48.5em;max-width:100%;display:flex;justify-content:space-between\"><div style=\"float:left;width:1.5em;font-size:1.3em;padding-top:2px;height:100%\">üí°</div><div style=\"flex:1;height:100%;padding-top:7px;padding-bottom:7px;\"><span style=\"color:#329609;font-weight:700;display:block\">GOOD TO KNOW</span>You can replace an existing column by simply using <code>DataFrame.withColumn()</code> and passing in the name of the column you want to replace as a first argument. Then, Spark will return a <code>DataFrame</code> where this column is replaced with the new value you supply.</div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6325d41a-c6bc-425e-9495-0262b7e6de41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- PurchaseDate: date (nullable = false)\n |-- HasPaid: boolean (nullable = false)\n |-- HasReceivedGood: boolean (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Setup of `simple_schema_df` - do not change anything here\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, DateType, BooleanType\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "simple_schema_data = [\n",
    "  (date(2023, 9, 12), True, True),\n",
    "  (date(2023, 4, 28), True, False),\n",
    "  (date(2022, 12, 10), False, None),\n",
    "  (date(2022, 10, 1), False, None)\n",
    "]\n",
    "\n",
    "simple_schema = StructType([\n",
    "  StructField(\"PurchaseDate\", DateType(), False),\n",
    "  StructField(\"HasPaid\", BooleanType(), False),\n",
    "  StructField(\"HasReceivedGood\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "simple_schema_df = spark.createDataFrame(simple_schema_data, schema=simple_schema)\n",
    "simple_schema_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e46cc7c3-567d-41d7-9bfb-1da89ecaba8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>PurchaseDate</th><th>HasPaid</th><th>HasReceivedGood</th></tr></thead><tbody><tr><td>2023-09-12</td><td>1</td><td>true</td></tr><tr><td>2023-04-28</td><td>1</td><td>false</td></tr><tr><td>2022-12-10</td><td>0</td><td>null</td></tr><tr><td>2022-10-01</td><td>0</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2023-09-12",
         1,
         true
        ],
        [
         "2023-04-28",
         1,
         false
        ],
        [
         "2022-12-10",
         0,
         null
        ],
        [
         "2022-10-01",
         0,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "PurchaseDate",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "HasPaid",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "HasReceivedGood",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create DataFrame `modified_schema_df` by modifying DataFrame `simple_schema_df`\n",
    "# so that it matches the specifications in the exercise \n",
    "\n",
    "# Your imports here\n",
    "from pyspark.sql.types import ByteType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "modified_schema_df = simple_schema_df.withColumn('PurchaseDate', col('PurchaseDate').cast('string')).withColumn('HasPaid', col('HasPaid').cast(ByteType()))\n",
    "display(modified_schema_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a70f3894-aa41-457c-a350-f2f60d35bd36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./answer_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0270ffd-332c-47f2-a62b-20d2247ccf41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data-mazing!\n"
     ]
    }
   ],
   "source": [
    "check_answer(modified_schema_df, exercise='changing_column_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56e2cfa8-2c19-4eab-ae35-a3d891d23baf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<span style=\"background-color:#FD6B131A;padding-top:5px;padding-left:10px;padding-right:10px;padding-bottom:5px;display:inline-block;border-radius:10px;border:2px #ffffff33 solid\">Please help me improve the course and click on an emoji to rate this exercise:<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSe2szXH5hep1KzDdMYZ8zdIOM9tVopVwMNL8A1-5e5I0WvAuw/formResponse?usp=pp_url&entry.1715120627=changing_column_type&entry.503063108=1&submit=Submit\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"margin-left:10px;border-top:2px #ca4d02 solid;border-bottom:2px #ca4d02 solid;border-left:2px #ca4d02 solid;border-right:2px #ca4d02 solid;padding-left:7px;padding-right:7px;border-radius:14px;color:#fff;background-color:#fd6b13\">üò©</span></a><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSe2szXH5hep1KzDdMYZ8zdIOM9tVopVwMNL8A1-5e5I0WvAuw/formResponse?usp=pp_url&entry.1715120627=changing_column_type&entry.503063108=2&submit=Submit\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"margin-left:10px;border-top:2px #ca4d02 solid;border-bottom:2px #ca4d02 solid;border-left:2px #ca4d02 solid;border-right:2px #ca4d02 solid;padding-left:7px;padding-right:7px;border-radius:14px;color:#fff;background-color:#fd6b13\">üòü</span></a><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSe2szXH5hep1KzDdMYZ8zdIOM9tVopVwMNL8A1-5e5I0WvAuw/formResponse?usp=pp_url&entry.1715120627=changing_column_type&entry.503063108=3&submit=Submit\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"margin-left:10px;border-top:2px #ca4d02 solid;border-bottom:2px #ca4d02 solid;border-left:2px #ca4d02 solid;border-right:2px #ca4d02 solid;padding-left:7px;padding-right:7px;border-radius:14px;color:#fff;background-color:#fd6b13\">üòê</span></a><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSe2szXH5hep1KzDdMYZ8zdIOM9tVopVwMNL8A1-5e5I0WvAuw/formResponse?usp=pp_url&entry.1715120627=changing_column_type&entry.503063108=4&submit=Submit\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"margin-left:10px;border-top:2px #ca4d02 solid;border-bottom:2px #ca4d02 solid;border-left:2px #ca4d02 solid;border-right:2px #ca4d02 solid;padding-left:7px;padding-right:7px;border-radius:14px;color:#fff;background-color:#fd6b13\">üòä</span></a><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSe2szXH5hep1KzDdMYZ8zdIOM9tVopVwMNL8A1-5e5I0WvAuw/formResponse?usp=pp_url&entry.1715120627=changing_column_type&entry.503063108=5&submit=Submit\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"margin-left:10px;border-top:2px #ca4d02 solid;border-bottom:2px #ca4d02 solid;border-left:2px #ca4d02 solid;border-right:2px #ca4d02 solid;padding-left:7px;padding-right:7px;border-radius:14px;color:#fff;background-color:#fd6b13\">üòÑ</span></a></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ed9b27c-d491-46c5-9c16-df1b6812e51d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Congratulations, you have already learned something new on your first day!"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "1_your_first_day_dataframe_fundamentals",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
